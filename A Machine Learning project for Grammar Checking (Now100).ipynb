{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ba5ad5a",
   "metadata": {},
   "source": [
    "# Grammatical Error Correction Using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904514b3",
   "metadata": {},
   "source": [
    "# Literature Survey "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e572452a",
   "metadata": {},
   "source": [
    "I have referred to this paper “Grammatical Error Checking Systems: A Review of Approaches and Emerging Directions” to do an extensive Literature Survey, which can be found in the below link.\n",
    "\n",
    "\"https://www.researchgate.net/publication/344160222_Recent_Trends_in_the_Use_of_Deep_Learning_Models_for_Grammar_Error_Handling\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18183606",
   "metadata": {},
   "source": [
    "# Understanding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf0eb60",
   "metadata": {},
   "source": [
    "Dataset - Lang-8 Corpus of Learner English\n",
    "\n",
    "Data source - \"https://docs.google.com/forms/d/e/1FAIpQLSflRX3h5QYxegivjHN7SJ194OxZ4XN_7Rt0cNpR2YbmNV-7Ag/viewform\"\n",
    "\n",
    "Credits - Tomoya Mizumoto, Mamoru Komachi, Masaaki Nagata and Yuji Matsumoto.\n",
    "Mining Revision Log of Language Learning SNS for Automated Japanese\n",
    "Error Correction of Second Language Learners. In Proceedings of the\n",
    "5th International Joint Conference on Natural Language Processing\n",
    "(IJCNLP), pp.147-155. Chiang Mai, Thailand, November 2011.\n",
    "\n",
    "Toshikazu Tajiri, Mamoru Komachi and Yuji Matsumoto. Tense and Aspect\n",
    "Error Correction for ESL Learners Using Global Context. In Proceedings\n",
    "of the 50th Annual Meeting of the Association for Computational\n",
    "Linguistics: Short Papers (oral), pp.198-202. Jeju Island, Korea, July 2012.\n",
    "\n",
    "The data is in M2 format, which consists of a line followed by S denotes an original sentence while a line followed by A indicates an edit annotation there are more than one annotation for an incorrect sentence\n",
    "\n",
    "Example data format:\n",
    "\n",
    "S I heard a sentence last night when I watched TV .\n",
    "A 8 9|||R:VERB:TENSE|||was watching|||REQUIRED|||-NONE-|||0\n",
    "\n",
    "S We ‘ve known each other for only half a year, but his lesson was a lot of fun.\n",
    "A 13 14|||R:NOUN:NUM|||lessons|||REQUIRED|||-NONE-|||0\n",
    "A 14 15|||R:VERB:SVA|||were|||REQUIRED|||-NONE-|||0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752f7189",
   "metadata": {},
   "source": [
    "Performence Metric:-\n",
    "\n",
    "GLEU score: To calculate the GLUE score we take all sub-sequences 1, 2, 3, or 4 tokens in output and target sequence, and then compute a recall, which is the ratio of the number of matching n-grams to the number of total n-grams in the target sequence, and a precision, which is the ratio of the number of matching n-grams to the number of total n-grams in the generated output sequence.\n",
    "GLEU score is simply the minimum of recall and precision.\n",
    "It ranges between 0 to 1,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003e37ab",
   "metadata": {},
   "source": [
    "# Exploritory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e07827",
   "metadata": {},
   "source": [
    "Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25609430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sb\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense,RNN,Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "import nltk.translate.bleu_score as bleu\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbf60998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(r\"D:\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e3af66",
   "metadata": {},
   "source": [
    "# M2 to CSV Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50b32225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Source - \"https://www.cl.cam.ac.uk/research/nl/bea2019st/data/corr_from_m2.py\"\n",
    "# Apply the edits of a single annotator to generate the corrected sentences.\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    this function stores the correct sentence line by line in as txt file with file name lang8.train.auto.bea19.m2\n",
    "    \"\"\"\n",
    "    \n",
    "    m2 = open(\"lang8.train.auto.bea19.m2\").read().strip().split(\"\\n\\n\")\n",
    "    out = open(\"lang8.train.auto.bea19.txt\", \"w\")\n",
    "    # Do not apply edits with these error types\n",
    "    skip = {\"noop\", \"UNK\", \"Um\"}\n",
    "    \n",
    "    for sent in m2:\n",
    "        sent = sent.split(\"\\n\")\n",
    "        cor_sent = sent[0].split()[1:] # Ignore \"S \"\n",
    "        edits = sent[1:]\n",
    "        offset = 0\n",
    "        for edit in edits:\n",
    "            edit = edit.split(\"|||\")\n",
    "            if edit[1] in skip: continue # Ignore certain edits\n",
    "            coder = int(edit[-1])\n",
    "            if coder != 0: continue # Ignore other coders\n",
    "            span = edit[0].split()[1:] # Ignore \"A \"\n",
    "            start = int(span[0])\n",
    "            end = int(span[1])\n",
    "            cor = edit[2].split()\n",
    "            cor_sent[start+offset:end+offset] = cor\n",
    "            offset = offset-(end-start)+len(cor)\n",
    "        out.write(\" \".join(cor_sent)+\"\\n\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d313ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorrect Sentences Preprocessing\n",
    "\n",
    "fl1 = open(\"lang8.train.auto.bea19.m2\",\"r\")\n",
    "sent1 = fl1.read()\n",
    "\n",
    "Each_Sent = sent1.split(\"\\n\\n\")\n",
    "\n",
    "Incorrect = []\n",
    "for i in range(len(Each_Sent)):\n",
    "    temp = Each_Sent[i].split(\"\\n\")\n",
    "    temp = temp[0]\n",
    "    temp = temp.split(\" \")\n",
    "    temp = temp[1:]# ignore S\n",
    "    temp = ' '.join(temp)\n",
    "    Incorrect.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2083c13",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'lang8.train.auto.bea19.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Correct Sentences Preprocessing\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m fl2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlang8.train.auto.bea19.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m sent2 \u001b[38;5;241m=\u001b[39m fl2\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      6\u001b[0m Correct \u001b[38;5;241m=\u001b[39m sent2\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'lang8.train.auto.bea19.txt'"
     ]
    }
   ],
   "source": [
    "# Correct Sentences Preprocessing\n",
    "\n",
    "fl2 = open(\"lang8.train.auto.bea19.txt\",\"r\")\n",
    "sent2 = fl2.read()\n",
    "\n",
    "Correct = sent2.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ffeb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing Correct and Incorrect sentence pair into dataframe\n",
    "df = pd.DataFrame()\n",
    "df[\"Correct\"] = Correct\n",
    "df[\"Incorrect\"] = Incorrect\n",
    "\n",
    "#store into csv file named data.csv\n",
    "df.to_csv(\"data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4059403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ffedbf",
   "metadata": {},
   "source": [
    "Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f9551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = []\n",
    "for i in range(len(data.values)):\n",
    "    if data.values[i][0] == data.values[i][1]:\n",
    "        index.append(i)\n",
    "            \n",
    "data = data.drop(index)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59467fce",
   "metadata": {},
   "source": [
    "# Missing Values Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93b5582",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3713af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows if contain null value\n",
    "# data.dropna(inplace=True)\n",
    "# data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98270c8",
   "metadata": {},
   "source": [
    "# Duplicate Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4861a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9703c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove if any\n",
    "# data.drop_duplicates(inplace=True)\n",
    "# data.reset_index(inplace=True,drop=True)\n",
    "# data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f180cb1d",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81df57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    \"\"\"\n",
    "    takes string as input and\n",
    "    removes characters inside (),{},[] and <>\n",
    "    removes characters like -+@#^/|*(){}$~`\n",
    "    we not not removing ,.!-:;\"' as these characters are present in english language \n",
    "    \"\"\"\n",
    "    text = re.sub('<.*>', '', text)\n",
    "    text = re.sub('\\(.*\\)', '', text)\n",
    "    text = re.sub('\\[.*\\]', '', text)\n",
    "    text = re.sub('{.*}', '', text)\n",
    "    text = re.sub(\"[-+@#^/|*(){}$~`<>=_]\",\"\",text)\n",
    "    text = text.replace(\"\\\\\",\"\")\n",
    "    text = re.sub(\"\\[\",\"\",text)\n",
    "    text = re.sub(\"\\]\",\"\",text)\n",
    "    text = re.sub(\"[0-9]\",\"\",text)\n",
    "    return text\n",
    "\n",
    "data[\"Correct\"] = data[\"Correct\"].apply(clean)\n",
    "data[\"Incorrect\"] = df[\"Incorrect\"].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7c7674",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8049a54c",
   "metadata": {},
   "source": [
    "# Length Of Correct Sentences Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527993f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentile(low,high,step,lst1):\n",
    "    \"\"\"\n",
    "    this function takes low, high, step size as input and prints percentiles accordingly\n",
    "    \"\"\"\n",
    "    for i in np.arange(low,high,step):\n",
    "        print(i,\"percentile is \",np.percentile(lst1, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81882baa",
   "metadata": {},
   "source": [
    "At Charecter Level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538e0261",
   "metadata": {},
   "source": [
    "For Correct Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575e9988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sen_to_char(sen):\n",
    "    return len([i for i in sen])\n",
    "\n",
    "Corr_length = data[\"Correct\"].apply(sen_to_char)\n",
    "Corr_length = list(Corr_length)\n",
    "len(Corr_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e0c0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile(0,101,10,Corr_length)\n",
    "\n",
    "print(\"***************************************************************\")\n",
    "\n",
    "percentile(90,101,1,Corr_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f87685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing those data points which have Correct sentence of length more than a certain value\n",
    "index = []\n",
    "for i in range(len(Corr_length)):\n",
    "    if Corr_length[i] > 100:\n",
    "        index.append(i)\n",
    "        \n",
    "data.drop(index,inplace=True)\n",
    "data.reset_index(inplace=True,drop=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29df915",
   "metadata": {},
   "source": [
    "For Incorrect Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31b7a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Incorr_length = data[\"Incorrect\"].apply(sen_to_char)\n",
    "Incorr_length = list(Incorr_length)\n",
    "len(Incorr_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230f3657",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile(0,101,10,Incorr_length)\n",
    "\n",
    "print(\"***********************************************************\")\n",
    "\n",
    "percentile(90,101,1,Incorr_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d098493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing those data points which have Incorrect sentence of length more than a certain value\n",
    "\n",
    "index = []\n",
    "for i in range(len(Incorr_length)):\n",
    "    if Incorr_length[i] > 100:\n",
    "        index.append(i)\n",
    "        \n",
    "data.drop(index,inplace=True)\n",
    "data.reset_index(inplace=True,drop=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f9fdf2",
   "metadata": {},
   "source": [
    "# At Word Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06f358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Corr_length = data[\"Correct\"].str.split().apply(len)\n",
    "Corr_length = list(Corr_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f143fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile(0,101,10,Corr_length)\n",
    "\n",
    "print(\"***************************************************************\")\n",
    "\n",
    "percentile(90,101,1,Corr_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265c97e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing those data points which have correct sentence of length more than a certain value\n",
    "\n",
    "index = []\n",
    "for i in range(len(Corr_length)):\n",
    "    if Corr_length[i] > 100:\n",
    "        index.append(i)\n",
    "        \n",
    "data.drop(index,inplace=True)\n",
    "data.reset_index(inplace=True,drop=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c65aae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Incorr_length = data[\"Incorrect\"].str.split().apply(len)\n",
    "Incorr_length = list(Incorr_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a307bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile(0,101,10,Incorr_length)\n",
    "\n",
    "print(\"***************************************************************\")\n",
    "\n",
    "percentile(90,101,1,Incorr_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359e6a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing those data points which have incorrect sentence of length more than a certain value\n",
    "\n",
    "index = []\n",
    "for i in range(len(Incorr_length)):\n",
    "    if Incorr_length[i] > 100:\n",
    "        index.append(i)\n",
    "        \n",
    "data.drop(index,inplace=True)\n",
    "data.reset_index(inplace=True,drop=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eec70bf",
   "metadata": {},
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a15c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_temp, cv = train_test_split(data, test_size=0.15)\n",
    "train, test = train_test_split(train_temp, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2498091d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store dataset into disk\n",
    "\n",
    "train.to_csv(\"train_word.csv\",index=False)\n",
    "cv.to_csv(\"cv_word.csv\",index=False)\n",
    "test.to_csv(\"test_word.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a77215e",
   "metadata": {},
   "source": [
    "# Unique words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f12b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train_word.csv\")\n",
    "cv_data = pd.read_csv(\"cv_word.csv\")\n",
    "test_data = pd.read_csv(\"test_word.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc92b54",
   "metadata": {},
   "source": [
    "Unique words in train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf16bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Incorrect sentences\n",
    "\n",
    "split_sent = np.array(train_data[\"Incorrect\"].str.split())\n",
    "\n",
    "unique = []\n",
    "for i in split_sent:\n",
    "    if type(i) == float:\n",
    "        continue\n",
    "    for j in i:\n",
    "        unique.append(j)\n",
    "\n",
    "unique_words_train_incorr = set(unique)\n",
    "print(\"total number of unique words in Incorrect sentences in train data are\",len(unique_words_train_incorr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81e408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Correct sentences\n",
    "\n",
    "split_sent = np.array(train_data[\"Correct\"].str.split())\n",
    "\n",
    "unique = []\n",
    "for i in split_sent:\n",
    "    for j in i:\n",
    "        unique.append(j)\n",
    "\n",
    "unique_words_train_corr = set(unique)\n",
    "print(\"total number of unique words in Correct sentences in train are\",len(unique_words_train_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a45f222",
   "metadata": {},
   "source": [
    "Unique words in cv dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cfca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Incorrect sentences\n",
    "\n",
    "split_sent = np.array(cv_data[\"Incorrect\"].str.split())\n",
    "\n",
    "unique = []\n",
    "for i in split_sent:\n",
    "    for j in i:\n",
    "        unique.append(j)\n",
    "\n",
    "unique_words_cv_incorr = set(unique)\n",
    "print(\"total number of unique words in Incorrect sentences in cv data are\",len(unique_words_cv_incorr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304a6535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Correct sentences\n",
    "\n",
    "split_sent = np.array(cv_data[\"Correct\"].str.split())\n",
    "\n",
    "unique = []\n",
    "for i in split_sent:\n",
    "    for j in i:\n",
    "        unique.append(j)\n",
    "\n",
    "unique_words_cv_corr = set(unique)\n",
    "print(\"total number of unique words in Correct sentences in cv data are\",len(unique_words_cv_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dbfd97",
   "metadata": {},
   "source": [
    "Unique words in test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e4b0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Incorrect sentences\n",
    "\n",
    "split_sent = np.array(test_data[\"Incorrect\"].str.split())\n",
    "\n",
    "unique = []\n",
    "for i in split_sent:\n",
    "    for j in i:\n",
    "        unique.append(j)\n",
    "\n",
    "unique_words_test_incorr = set(unique)\n",
    "print(\"total number of unique words in Incorrect sentences in test data are\",len(unique_words_cv_incorr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc2115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Correct sentences\n",
    "\n",
    "split_sent = np.array(test_data[\"Incorrect\"].str.split())\n",
    "\n",
    "unique = []\n",
    "for i in split_sent:\n",
    "    for j in i:\n",
    "        unique.append(j)\n",
    "\n",
    "unique_words_test_corr = set(unique)\n",
    "print(\"total number of unique words in Correct sentences in test are\",len(unique_words_test_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2268554d",
   "metadata": {},
   "source": [
    "# Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37161dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = pd.read_csv(\"train.csv\")\n",
    "cv_file = pd.read_csv(\"cv.csv\")\n",
    "test_file = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9017c840",
   "metadata": {},
   "source": [
    "We will add $ to each sentence which will be input to decoder also We will add @ to each sentence which will be output of decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6713eda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file[\"Correct_inp\"] = \"$\" + train_file[\"Correct\"].astype(str) # $ denotes start of sentence\n",
    "train_file[\"Correct_out\"] = train_file[\"Correct\"].astype(str) + \"@\" # @ denotes end of sentence\n",
    "\n",
    "cv_file[\"Correct_inp\"] = \"$\" + cv_file[\"Correct\"].astype(str)\n",
    "cv_file[\"Correct_out\"] = cv_file[\"Correct\"].astype(str) + \"@\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a092e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code reference https://stackoverflow.com/questions/45735070/keras-text-preprocessing-saving-tokenizer-object-to-file-for-scoring\n",
    "# loading saved tokenizer\n",
    "\n",
    "with open(\"tokenizer_incorr.pickle\",\"rb\") as temp1:\n",
    "    tokenizer_incorr = pickle.load(temp1)\n",
    "    \n",
    "with open(\"tokenizer_corr_inp.pickle\",\"rb\") as temp2:\n",
    "    tokenizer_corr_inp = pickle.load(temp2)\n",
    "    \n",
    "with open(\"tokenizer_corr_out.pickle\",\"rb\") as temp3:\n",
    "    tokenizer_corr_out = pickle.load(temp3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae1e6fc",
   "metadata": {},
   "source": [
    "Tokenization Sentences For Feeding To Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22025c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_incorr = Tokenizer(filters=\"\",char_level=True,lower=False)\n",
    "#tokenizer_incorr.fit_on_texts(train[\"incorrect\"].values)\n",
    "\n",
    "Incorr_train = np.array(tokenizer_incorr.texts_to_sequences(train_file[\"Incorrect\"].values))\n",
    "Incorr_cv = np.array(tokenizer_incorr.texts_to_sequences(cv_file[\"Incorrect\"].values))\n",
    "print(\"Vocab size of Incorrrect sentences is\",len(tokenizer_incorr.word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b491b4f2",
   "metadata": {},
   "source": [
    "Tokenizing Senetence For Feeding To Decoder As Inpput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5e0be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_corr_inp = Tokenizer(filters=\"\",char_level=True,lower=False)\n",
    "#tokenizer_corr_inp.fit_on_texts(train[\"correct_inp\"].values)\n",
    "\n",
    "Corr_train_inp = np.array(tokenizer_corr_inp.texts_to_sequences(train_file[\"Correct_inp\"].values))\n",
    "Corr_cv_inp = np.array(tokenizer_corr_inp.texts_to_sequences(cv_file[\"Correct_inp\"].values))\n",
    "print(\"vocab size of Corrrect sentences is\",len(tokenizer_corr_inp.word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06825a81",
   "metadata": {},
   "source": [
    "Tokenizing Senetence That Will Be Output Of Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481dac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_corr_out = Tokenizer(filters=\"\",char_level=True,lower=False)\n",
    "# tokenizer_corr_out.fit_on_texts(train[\"correct_out\"].values)\n",
    "\n",
    "Corr_train_out = np.array(tokenizer_corr_out.texts_to_sequences(train_file[\"Correct_out\"].values))\n",
    "Corr_cv_out = np.array(tokenizer_corr_inp.texts_to_sequences(cv_file[\"Correct_out\"].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f078f84",
   "metadata": {},
   "source": [
    "Padding Train, Cv, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dc1400",
   "metadata": {},
   "outputs": [],
   "source": [
    "Incorr_train = np.array(pad_sequences(Incorr_train,maxlen=110, padding=\"post\", truncating='post'))\n",
    "Corr_train_inp = np.array(pad_sequences(Corr_train_inp, maxlen=110, padding=\"post\", truncating='post'))\n",
    "Corr_train_out = np.array(pad_sequences(Corr_train_out, maxlen=110, padding=\"post\", truncating='post'))\n",
    "\n",
    "Incorr_cv = np.array(pad_sequences(Incorr_cv, maxlen=110, padding=\"post\", truncating='post'))\n",
    "Corr_cv_inp = np.array(pad_sequences(Corr_cv_inp, maxlen=110, padding=\"post\", truncating='post'))\n",
    "Corr_cv_out = np.array(pad_sequences(Corr_cv_out, maxlen=110, padding=\"post\", truncating='post'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bd96be",
   "metadata": {},
   "source": [
    "# MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3654de",
   "metadata": {},
   "source": [
    "Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c26b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code taken from \"https://github.com/mridul1012/Grammatical-Error-Correction-with-Neural-Networks/tree/main\"\n",
    "\n",
    "############################## Encoder class #############################################################\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Encoder model -- That takes a input sequence and returns encoder-outputs,encoder_final_state_h,encoder_final_state_c\n",
    "    '''\n",
    "\n",
    "    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.lstm_size = lstm_size\n",
    "        self.embedding = Embedding(input_dim=inp_vocab_size, output_dim=embedding_size, input_length=input_length,\n",
    "                           mask_zero=True,name=\"embedding_layer_encoder\")\n",
    "        self.lstmcell = tf.keras.layers.LSTMCell(lstm_size)\n",
    "        self.encoder_lstm = RNN(self.lstmcell,return_sequences=True, return_state=True)\n",
    "\n",
    "\n",
    "    def call(self,input_sequence,states):\n",
    "        '''\n",
    "          This function takes a sequence input and the initial states of the encoder.\n",
    "          Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to encoder_lstm\n",
    "          returns -- encoder_output, last time step's hidden and cell state\n",
    "        '''\n",
    "\n",
    "        output1 = self.embedding(input_sequence)\n",
    "        enco_output, enco_state_h, enco_state_c = self.encoder_lstm(output1, initial_state=states)\n",
    "        return enco_output, enco_state_h, enco_state_c\n",
    "\n",
    "    \n",
    "    def initialize_states(self,batch_size):\n",
    "\n",
    "        initial_hidden_state = tf.zeros([batch_size,self.lstm_size])\n",
    "        initial_cell_state = tf.zeros([batch_size,self.lstm_size])\n",
    "        \n",
    "        return [initial_hidden_state,initial_cell_state]\n",
    "\n",
    "############################## Decoder class #############################################################\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Encoder model -- That takes a input sequence and returns output sequence\n",
    "    '''\n",
    "\n",
    "    def __init__(self,out_vocab_size,embedding_size,lstm_size,input_length):\n",
    "\n",
    "        super().__init__()\n",
    "        self.lstm_size = lstm_size\n",
    "        self.embedding = Embedding(input_dim=out_vocab_size, output_dim=embedding_size, input_length=input_length,\n",
    "                           mask_zero=True,name=\"embedding_layer_encoder\")\n",
    "        self.lstmcell = tf.keras.layers.LSTMCell(lstm_size)\n",
    "        self.decoder_lstm = RNN(self.lstmcell,return_sequences=True, return_state=True)\n",
    "\n",
    "    def call(self,target_sequence,initial_states):\n",
    "        '''\n",
    "          This function takes a sequence input and the initial states of the encoder.\n",
    "          Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to decoder_lstm\n",
    "        \n",
    "          returns -- decoder_output,decoder_final_state_h,decoder_final_state_c\n",
    "        '''\n",
    "        output2 = self.embedding(target_sequence)\n",
    "        deco_output, deco_state_h, deco_state_c = self.decoder_lstm(output2, initial_state=initial_states)\n",
    "      \n",
    "        return deco_output, deco_state_h, deco_state_c\n",
    "\n",
    "##############################encoder decoder class#############################################################    \n",
    "    \n",
    "qw_state = 0\n",
    "class Encoder_decoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,inp_vocab_size,out_vocab_size,embedding_size,lstm_size,input_length,batch_size,*args):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(inp_vocab_size,embedding_size,lstm_size,input_length)\n",
    "        #print(\"output vocab size in encoder decoder class\",out_vocab_size)\n",
    "        self.decoder = Decoder(out_vocab_size,embedding_size,lstm_size,input_length)\n",
    "        self.dense   = Dense(out_vocab_size)#, activation='softmax')\n",
    "        self.batch = batch_size\n",
    "    \n",
    "    \n",
    "    def call(self,data,*args):\n",
    "        '''\n",
    "        A. Pass the input sequence to Encoder layer -- Return encoder_output,encoder_final_state_h,encoder_final_state_c\n",
    "        B. Pass the target sequence to Decoder layer with intial states as encoder_final_state_h,encoder_final_state_C\n",
    "        C. Pass the decoder_outputs into Dense layer \n",
    "        \n",
    "        Return decoder_outputs\n",
    "        '''\n",
    "        \n",
    "        input,output = data[0], data[1]\n",
    "        # initializing initial states of encoder\n",
    "        l = self.encoder.initialize_states(self.batch)\n",
    "        qw_state = l\n",
    "        #print(\"WE ARE INITIALIZING encoder WITH initial STATES as zeroes :\",l[0].shape, l[1].shape)\n",
    "        #print(\"hello\")\n",
    "        encoder_output,encoder_final_state_h,encoder_final_state_c = self.encoder(input,l)\n",
    "        #print(\"ENCODER ==> OUTPUT SHAPE\",encoder_output.shape)\n",
    "        #print(\"ENCODER ==> HIDDEN STATE SHAPE\",encoder_final_state_h.shape)\n",
    "        #print(\"ENCODER ==> CELL STATE SHAPE\", encoder_final_state_c.shape)\n",
    "        #print(\"hi\")\n",
    "        m = list((encoder_final_state_h,encoder_final_state_c))\n",
    "        decoder_output,decoder_final_state_h,decoder_final_state_c = self.decoder(output,m)\n",
    "        #print(\"decoder OUTPUT SHAPE\",decoder_output.shape)\n",
    "        #print(\"type of decoder output is \",type(decoder_output))\n",
    "        #x = self.flatten(decoder_output)\n",
    "        #print(\"shape of x \",x.shape)\n",
    "        qw_output = self.dense(decoder_output)\n",
    "        #print(\"FINAL OUTPUT SHAPE\",qw_output.shape)\n",
    "        return qw_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f266143",
   "metadata": {},
   "source": [
    "# Encoder Decoder Model With Character Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840a1236",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_vocab_size = 63\n",
    "out_vocab_size = 64\n",
    "embedding_dim=100\n",
    "input_length=110\n",
    "lstm_size=256\n",
    "batch_size=1024\n",
    "#model = Encoder_decoder(inp_vocab_size,out_vocab_size,embedding_dim,lstm_size,input_length,batch_size)\n",
    "# custom loss function\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "#defining custom loss function which will not consider loss for padded zeroes\n",
    "# code taken from attention assignment\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)\n",
    "#model.compile(optimizer=optimizer,loss=loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2064e1",
   "metadata": {},
   "source": [
    "Model Traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6373e6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=[Incorr_train, Corr_train_inp], y=Corr_train_out, epochs=10, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356450c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61713e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Encoder_decoder(inp_vocab_size,out_vocab_size,embedding_dim,lstm_size,input_length,batch_size)\n",
    "model.compile(optimizer=optimizer,loss=loss_function)\n",
    "model.train_on_batch([Incorr_train[:1024],Corr_train_inp[:1024]],Corr_train_out[:1024])\n",
    "\n",
    "# Load the state of the old model\n",
    "\n",
    "model.load_weights('enco_dec_char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08387f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_dict = tokenizer_corr_out.word_index\n",
    "inv_corr = {v: k for k, v in corr_dict.items()}\n",
    "\n",
    "def predict(input_sentence):\n",
    "    \"\"\"\n",
    "    this function takes incorrect input sentences s input and retirns correct sentences\n",
    "    \"\"\"\n",
    "    input_sentence = tokenizer_incorr.texts_to_sequences([input_sentence])\n",
    "    initial_hidden_state = tf.zeros([1,256])\n",
    "    initial_cell_state = tf.zeros([1,256])\n",
    "    qwst = [initial_hidden_state,initial_cell_state]\n",
    "    pred_total = []\n",
    "    enc_output, enc_state_h, enc_state_c = model.layers[0](np.expand_dims(input_sentence[0],0),qwst)\n",
    "    states_values = [enc_state_h, enc_state_c]\n",
    "    pred = []\n",
    "    sentence = []\n",
    "    cur_vec = np.array([[16]])#np.ones((1, 1),dtype='int')\n",
    "    for i in range(110):\n",
    "        dec_output, dec_state_h, dec_state_c = model.layers[1](cur_vec,states_values)\n",
    "        infe_output=model.layers[2](dec_output)\n",
    "        states_values = [dec_state_h, dec_state_c]\n",
    "        cur_vec = np.reshape(np.argmax(infe_output), (1, 1))\n",
    "        if inv_corr[cur_vec[0][0]] == '@':\n",
    "            break\n",
    "            #print(\"at time step \",i,\" the word is \", cur_vec)\n",
    "        pred.append(cur_vec[0][0])\n",
    "    for i in pred:\n",
    "        sentence.append(inv_corr[i])\n",
    "    #return pred\n",
    "    return \"\".join(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27120bcd",
   "metadata": {},
   "source": [
    "# GLUE Score on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708a0770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "\n",
    "gleu_score_test = 0\n",
    "length = 1000\n",
    "\n",
    "for i in range(length):\n",
    "    reference = [test_file[\"Correct\"].values[i:i+1][0].split()]\n",
    "    candidate = predict(test_file[\"Incorrect\"].values[i:i+1][0]).split()\n",
    "    gleu_score_test = gleu_score_test + sentence_gleu(reference, candidate)\n",
    "print(\"Final GLEU Score on Test data are\",gleu_score_test/length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d79d0a",
   "metadata": {},
   "source": [
    "Result Prediction On Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cc83eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted Sentences\n",
    "\n",
    "for i in train_file[\"Incorrect\"].values[:10]:\n",
    "  print(predict(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd771d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual Sentences\n",
    "\n",
    "train_file[\"Correct\"].values[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf122ea",
   "metadata": {},
   "source": [
    "Result Prediction On CV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688aeabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted Sentences\n",
    "\n",
    "for i in cv_file[\"Incorrect\"].values[:10]:\n",
    "  print(predict(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c49603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual Sentences\n",
    "\n",
    "cv_file[\"Correct\"].values[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d1f489",
   "metadata": {},
   "source": [
    "Result Prediction On Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55914380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted Sentences\n",
    "\n",
    "for i in test_file[\"Incorrect\"].values[:10]:\n",
    "  print(predict(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8618c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual Sentences\n",
    "\n",
    "test_file[\"Correct\"].values[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
